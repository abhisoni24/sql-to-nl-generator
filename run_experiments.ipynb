{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# SQL Generation LLM Experiment Runner\n",
                "\n",
                "This notebook runs comprehensive experiments to evaluate LLMs on SQL generation tasks with perturbation testing.\n",
                "\n",
                "**Features:**\n",
                "- Multi-model testing (API + Local vLLM)\n",
                "- Automatic rate limiting\n",
                "- Comprehensive analysis and visualization\n",
                "- GPU acceleration for vLLM models\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!git clone https://github.com/abhisoni24/sql-to-nl-generator.git\n",
                "!ls\n",
                "!pwd\n",
                "!pip install -r requirements.txt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "/content\n",
                        "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
                        "\u001b[0m‚úì Project root: /content\n",
                        "‚úì Python version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n"
                    ]
                }
            ],
            "source": [
                "# Import required libraries\n",
                "import sys\n",
                "import os\n",
                "from pathlib import Path\n",
                "import json\n",
                "from datetime import datetime\n",
                "import yaml\n",
                "\n",
                "# Add project to path\n",
                "project_root = Path.cwd()\n",
                "if str(project_root) not in sys.path:\n",
                "    sys.path.insert(0, str(project_root))\n",
                "\n",
                "print(f\"‚úì Project root: {project_root}\")\n",
                "print(f\"‚úì Python version: {sys.version}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "API Key Status:\n",
                        "  GEMINI_API_KEY: ‚úó Missing\n",
                        "  OPENAI_API_KEY: ‚úó Missing\n",
                        "  CLAUDE_API_KEY: ‚úó Missing\n"
                    ]
                }
            ],
            "source": [
                "# Load environment variables (API keys)\n",
                "from dotenv import load_dotenv\n",
                "load_dotenv()\n",
                "from tqdm.auto import tqdm\n",
                "import traceback\n",
                "\n",
                "# Check for API keys\n",
                "api_keys = {\n",
                "    'GEMINI_API_KEY': os.getenv('GEMINI_API_KEY'),\n",
                "    'OPENAI_API_KEY': os.getenv('OPENAI_API_KEY'),\n",
                "    'CLAUDE_API_KEY': os.getenv('CLAUDE_API_KEY')\n",
                "}\n",
                "\n",
                "print(\"API Key Status:\")\n",
                "for key, value in api_keys.items():\n",
                "    status = \"‚úì Set\" if value else \"‚úó Missing\"\n",
                "    print(f\"  {key}: {status}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "ename": "ModuleNotFoundError",
                    "evalue": "No module named 'src'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
                        "\u001b[0;32m/tmp/ipython-input-705686014.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import experiment components\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mharness\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfigLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mharness\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExecutionEngine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚úì Experiment modules loaded successfully\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'",
                        "",
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
                    ]
                }
            ],
            "source": [
                "# Import experiment components\n",
                "from src.harness.config import ConfigLoader\n",
                "from src.harness.core.execution import ExecutionEngine\n",
                "\n",
                "print(\"‚úì Experiment modules loaded successfully\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration paths\n",
                "CONFIG_FILE = \"experiments.yaml\"\n",
                "DATASET_FILE = \"dataset/current/nl_social_media_queries.json\"  # Full dataset\n",
                "# DATASET_FILE = \"dataset/current/mini_test_dataset.json\"  # Mini dataset for testing\n",
                "\n",
                "OUTPUT_DIR = \"experiment_logs\"\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "\n",
                "# Generate run ID\n",
                "RUN_ID = f\"experiment_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
                "OUTPUT_FILE = f\"{OUTPUT_DIR}/{RUN_ID}.jsonl\"\n",
                "\n",
                "print(f\"Configuration:\")\n",
                "print(f\"  Config file: {CONFIG_FILE}\")\n",
                "print(f\"  Dataset: {DATASET_FILE}\")\n",
                "print(f\"  Output file: {OUTPUT_FILE}\")\n",
                "print(f\"  Run ID: {RUN_ID}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load experiment configuration\n",
                "experiments = ConfigLoader.load_experiments(CONFIG_FILE)\n",
                "\n",
                "print(f\"\\nAvailable models ({len(experiments)}):\")\n",
                "for i, exp in enumerate(experiments, 1):\n",
                "    rate_info = \"No limit\" if not exp.rate_limit else f\"{exp.rate_limit.get('requests_per_minute', 'N/A')} req/min\"\n",
                "    print(f\"  {i}. {exp.name} ({exp.adapter_type}) - {rate_info}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Model Selection\n",
                "\n",
                "Select which models to run experiments on:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model selection\n",
                "# Options:\n",
                "# - 'all': Run all models\n",
                "# - List of model names: ['gemini-pro', 'gpt-4']\n",
                "# - List of indices: [0, 1, 2]\n",
                "\n",
                "SELECTED_MODELS = 'all'  # Run all models\n",
                "# SELECTED_MODELS = ['gemini-pro', 'local-qwen0.5b']  # Specific models\n",
                "# SELECTED_MODELS = [0, 3]  # By index (0-based)\n",
                "\n",
                "# Process selection\n",
                "if SELECTED_MODELS == 'all':\n",
                "    selected_experiments = experiments\n",
                "elif isinstance(SELECTED_MODELS, list):\n",
                "    if all(isinstance(x, int) for x in SELECTED_MODELS):\n",
                "        # By index\n",
                "        selected_experiments = [experiments[i] for i in SELECTED_MODELS]\n",
                "    else:\n",
                "        # By name\n",
                "        selected_experiments = [e for e in experiments if e.name in SELECTED_MODELS]\n",
                "else:\n",
                "    selected_experiments = experiments\n",
                "\n",
                "print(f\"\\nüìä Selected {len(selected_experiments)} model(s) for experiment:\")\n",
                "for exp in selected_experiments:\n",
                "    print(f\"  ‚úì {exp.name}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Run Experiments\n",
                "\n",
                "Execute experiments for selected models with rate limiting and error handling."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run experiments with enhanced progress tracking\n",
                "import uuid\n",
                "from IPython.display import clear_output, display, HTML\n",
                "from tqdm.auto import tqdm\n",
                "import traceback\n",
                "\n",
                "results = {}\n",
                "\n",
                "# Outer progress bar for model experiments\n",
                "model_pbar = tqdm(selected_experiments, desc=\"Running Experiments\", unit=\"model\", position=0, leave=True)\n",
                "\n",
                "for i, experiment in enumerate(model_pbar, 1):\n",
                "    model_pbar.set_description(f\"Experiment {i}/{len(selected_experiments)}: {experiment.name}\")\n",
                "    \n",
                "    print(f\"\\\\n{'='*70}\")\n",
                "    print(f\"Running Experiment {i}/{len(selected_experiments)}: {experiment.name}\")\n",
                "    print(f\"{'='*70}\")\n",
                "    \n",
                "    try:\n",
                "        # Create adapter\n",
                "        adapter = ConfigLoader.get_adapter(experiment)\n",
                "        \n",
                "        # Create execution engine with rate limiting\n",
                "        engine = ExecutionEngine(\n",
                "            adapter=adapter,\n",
                "            run_id=RUN_ID,\n",
                "            output_path=OUTPUT_FILE,\n",
                "            rate_limit_config=experiment.rate_limit\n",
                "        )\n",
                "        \n",
                "        # Run experiment (this will show nested progress bars)\n",
                "        start_time = datetime.now()\n",
                "        engine.execute_experiment(DATASET_FILE)\n",
                "        end_time = datetime.now()\n",
                "        \n",
                "        duration = (end_time - start_time).total_seconds()\n",
                "        \n",
                "        results[experiment.name] = {\n",
                "            'status': 'success',\n",
                "            'duration': duration,\n",
                "            'start': start_time,\n",
                "            'end': end_time\n",
                "        }\n",
                "        \n",
                "        print(f\"\\\\n‚úì Completed in {duration:.1f} seconds\")\n",
                "        model_pbar.set_postfix_str(f\"‚úì Success ({duration:.0f}s)\")\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"\\\\n‚úó Failed: {str(e)}\")\n",
                "        print(\"\\\\nFull traceback:\")\n",
                "        traceback.print_exc()\n",
                "        \n",
                "        results[experiment.name] = {\n",
                "            'status': 'failed',\n",
                "            'error': str(e),\n",
                "            'traceback': traceback.format_exc()\n",
                "        }\n",
                "        model_pbar.set_postfix_str(\"‚úó Failed\")\n",
                "        continue\n",
                "\n",
                "model_pbar.close()\n",
                "\n",
                "print(f\"\\\\n{'='*70}\")\n",
                "print(\"All experiments completed!\")\n",
                "print(f\"{'='*70}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Summary of experiment runs\n",
                "import pandas as pd\n",
                "\n",
                "summary_data = []\n",
                "for model, result in results.items():\n",
                "    if result['status'] == 'success':\n",
                "        summary_data.append({\n",
                "            'Model': model,\n",
                "            'Status': '‚úì Success',\n",
                "            'Duration (s)': f\"{result['duration']:.1f}\",\n",
                "            'Start': result['start'].strftime('%H:%M:%S'),\n",
                "            'End': result['end'].strftime('%H:%M:%S')\n",
                "        })\n",
                "    else:\n",
                "        summary_data.append({\n",
                "            'Model': model,\n",
                "            'Status': '‚úó Failed',\n",
                "            'Duration (s)': 'N/A',\n",
                "            'Start': 'N/A',\n",
                "            'End': result.get('error', 'Unknown error')\n",
                "        })\n",
                "\n",
                "summary_df = pd.DataFrame(summary_data)\n",
                "display(HTML(\"<h3>Experiment Summary</h3>\"))\n",
                "display(summary_df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Analyze Results\n",
                "\n",
                "Generate comprehensive analysis with visualizations and statistics."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run analysis\n",
                "from src.harness.core.analyze_results import ExperimentAnalyzer\n",
                "\n",
                "ANALYSIS_DIR = f\"{OUTPUT_DIR}/{RUN_ID}_analysis\"\n",
                "\n",
                "print(f\"Running analysis on: {OUTPUT_FILE}\")\n",
                "print(f\"Output directory: {ANALYSIS_DIR}\")\n",
                "\n",
                "analyzer = ExperimentAnalyzer(OUTPUT_FILE, ANALYSIS_DIR)\n",
                "analyzer.run_full_analysis()\n",
                "\n",
                "print(f\"\\n‚úì Analysis complete!\")\n",
                "print(f\"  - Figures: {ANALYSIS_DIR}/figures/\")\n",
                "print(f\"  - Tables: {ANALYSIS_DIR}/tables/\")\n",
                "print(f\"  - Summary: {ANALYSIS_DIR}/EXECUTIVE_SUMMARY.md\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. View Results\n",
                "\n",
                "Display key metrics and visualizations inline."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display executive summary\n",
                "with open(f\"{ANALYSIS_DIR}/EXECUTIVE_SUMMARY.md\", 'r') as f:\n",
                "    summary = f.read()\n",
                "\n",
                "from IPython.display import Markdown\n",
                "display(Markdown(summary))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display key metrics\n",
                "import matplotlib.pyplot as plt\n",
                "from IPython.display import Image\n",
                "\n",
                "# Model accuracy comparison\n",
                "print(\"Model Accuracy Comparison:\")\n",
                "display(Image(filename=f\"{ANALYSIS_DIR}/figures/model_accuracy_comparison.png\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Perturbation robustness\n",
                "print(\"Perturbation Robustness Analysis:\")\n",
                "display(Image(filename=f\"{ANALYSIS_DIR}/figures/perturbation_accuracy.png\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model vs Perturbation heatmap\n",
                "print(\"Model Performance Heatmap (Model √ó Perturbation):\")\n",
                "display(Image(filename=f\"{ANALYSIS_DIR}/figures/model_perturbation_heatmap.png\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load and display detailed statistics\n",
                "model_perf = pd.read_csv(f\"{ANALYSIS_DIR}/tables/model_performance.csv\", index_col=0)\n",
                "display(HTML(\"<h3>Detailed Model Performance</h3>\"))\n",
                "display(model_perf)\n",
                "\n",
                "pert_perf = pd.read_csv(f\"{ANALYSIS_DIR}/tables/perturbation_performance.csv\", index_col=0)\n",
                "display(HTML(\"<h3>Perturbation Performance</h3>\"))\n",
                "display(pert_perf)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Export Results\n",
                "\n",
                "Package results for sharing or further analysis."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create archive of results\n",
                "import shutil\n",
                "import tarfile\n",
                "\n",
                "archive_name = f\"{OUTPUT_DIR}/{RUN_ID}_results.tar.gz\"\n",
                "\n",
                "with tarfile.open(archive_name, 'w:gz') as tar:\n",
                "    tar.add(OUTPUT_FILE, arcname=f\"{RUN_ID}.jsonl\")\n",
                "    tar.add(ANALYSIS_DIR, arcname=f\"{RUN_ID}_analysis\")\n",
                "\n",
                "archive_size = os.path.getsize(archive_name) / (1024 * 1024)  # MB\n",
                "print(f\"‚úì Results archived to: {archive_name}\")\n",
                "print(f\"  Size: {archive_size:.2f} MB\")\n",
                "print(f\"\\nüì¶ Ready for download or transfer!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Quick Commands\n",
                "\n",
                "Useful commands for managing experiments on the VM."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check GPU availability (for vLLM)\n",
                "import subprocess\n",
                "\n",
                "try:\n",
                "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
                "    print(\"GPU Status:\")\n",
                "    print(result.stdout)\n",
                "except FileNotFoundError:\n",
                "    print(\"‚ö†Ô∏è  NVIDIA GPU not detected or nvidia-smi not available\")\n",
                "    print(\"   vLLM will use CPU (much slower)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# List all result files\n",
                "result_files = list(Path(OUTPUT_DIR).glob(\"*.jsonl\"))\n",
                "print(f\"Available result files ({len(result_files)}):\")\n",
                "for f in sorted(result_files, reverse=True):\n",
                "    size = f.stat().st_size / 1024  # KB\n",
                "    print(f\"  {f.name} ({size:.1f} KB)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Notes\n",
                "\n",
                "### For GPU VMs:\n",
                "1. Ensure CUDA is installed and configured\n",
                "2. vLLM will automatically use GPU if available\n",
                "3. Monitor GPU usage with `nvidia-smi` during experiments\n",
                "\n",
                "### Rate Limiting:\n",
                "- Configured in `experiments.yaml`\n",
                "- Automatically applied per model\n",
                "- Local vLLM models have no rate limits\n",
                "\n",
                "### Troubleshooting:\n",
                "- **API Errors**: Check API keys in `.env` file\n",
                "- **Model Not Found**: Verify model IDs in `experiments.yaml`\n",
                "- **Out of Memory**: Reduce batch size in ExecutionEngine\n",
                "- **Slow Execution**: Check rate limits or use faster models\n",
                "\n",
                "### Re-running Analysis:\n",
                "```python\n",
                "# To re-analyze existing results:\n",
                "analyzer = ExperimentAnalyzer(\n",
                "    \"experiment_logs/your_file.jsonl\",\n",
                "    \"experiment_logs/your_analysis_dir\"\n",
                ")\n",
                "analyzer.run_full_analysis()\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "sqlGen",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
